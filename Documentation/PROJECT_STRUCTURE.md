# Project Structure

Complete file structure for the RAG Evaluation System.

```
rag-evaluation/
â”‚
â”œâ”€â”€ ğŸ“„ Core Application Files
â”‚   â”œâ”€â”€ evaluator.py              # Main evaluation logic and RAGRetrievalEvaluator class
â”‚   â”œâ”€â”€ config.py                 # Configuration management (LLM, Phoenix, Evaluation)
â”‚   â””â”€â”€ utils.py                  # Utility functions (metrics, I/O, analysis)
â”‚
â”œâ”€â”€ ğŸ“š Documentation
â”‚   â”œâ”€â”€ README.md                 # Complete project documentation
â”‚   â”œâ”€â”€ QUICKSTART.md             # 5-minute getting started guide
â”‚   â”œâ”€â”€ PROJECT_STRUCTURE.md      # This file
â”‚   â””â”€â”€ LICENSE                   # MIT License (create if needed)
â”‚
â”œâ”€â”€ ğŸ§ª Testing & Quality
â”‚   â”œâ”€â”€ test_evaluator.py         # Comprehensive unit tests
â”‚   â”œâ”€â”€ .github/
â”‚   â”‚   â””â”€â”€ workflows/
â”‚   â”‚       â””â”€â”€ ci.yml            # GitHub Actions CI/CD pipeline
â”‚   â””â”€â”€ .flake8                   # Linter configuration (optional)
â”‚
â”œâ”€â”€ ğŸ³ Docker & Deployment
â”‚   â”œâ”€â”€ Dockerfile                # Docker image definition
â”‚   â”œâ”€â”€ docker-compose.yml        # Multi-container setup
â”‚   â””â”€â”€ .dockerignore             # Docker ignore patterns (create if needed)
â”‚
â”œâ”€â”€ ğŸ“¦ Package Management
â”‚   â”œâ”€â”€ requirements.txt          # Python dependencies
â”‚   â”œâ”€â”€ setup.py                  # Package installation script
â”‚   â”œâ”€â”€ Makefile                  # Convenient command shortcuts
â”‚   â””â”€â”€ pyproject.toml            # Modern Python packaging (optional)
â”‚
â”œâ”€â”€ âš™ï¸ Configuration
â”‚   â”œâ”€â”€ .env                      # Environment variables (create from .env.example)
â”‚   â”œâ”€â”€ .env.example              # Environment template
â”‚   â””â”€â”€ .gitignore                # Git ignore patterns
â”‚
â”œâ”€â”€ ğŸ“Š Example & Usage
â”‚   â”œâ”€â”€ example_usage.py          # 6 detailed usage examples
â”‚   â””â”€â”€ notebooks/                # Jupyter notebooks (optional)
â”‚       â”œâ”€â”€ tutorial.ipynb
â”‚       â””â”€â”€ advanced_usage.ipynb
â”‚
â”œâ”€â”€ ğŸ“ Data Directories
â”‚   â”œâ”€â”€ data/                     # Input data (create manually)
â”‚   â”‚   â”œâ”€â”€ queries.txt
â”‚   â”‚   â”œâ”€â”€ ground_truth.csv
â”‚   â”‚   â””â”€â”€ custom_dataset/
â”‚   â”‚
â”‚   â””â”€â”€ results/                  # Output files (auto-created)
â”‚       â”œâ”€â”€ retrieval_evaluation_results.csv
â”‚       â”œâ”€â”€ evaluation_metrics.csv
â”‚       â”œâ”€â”€ evaluation_report.html
â”‚       â””â”€â”€ *.json
â”‚
â””â”€â”€ ğŸ”§ Development (Optional)
    â”œâ”€â”€ .vscode/                  # VS Code settings
    â”‚   â””â”€â”€ settings.json
    â”œâ”€â”€ .pre-commit-config.yaml   # Pre-commit hooks
    â””â”€â”€ tox.ini                   # Multi-environment testing
```

## File Descriptions

### Core Application Files

#### `evaluator.py` (Main Module)
- **Classes:**
  - `RetrievalResult`: Data class for storing evaluation results
  - `EvaluationMetrics`: Data class for metrics (precision, recall, etc.)
  - `RAGRetrievalEvaluator`: Main evaluator using LangChain + Together AI
  - `PhoenixRAGTracer`: Phoenix integration for tracing
- **Functions:**
  - `create_sample_data()`: Generate sample data for testing
  - `main()`: Entry point for CLI execution

#### `config.py` (Configuration)
- **Classes:**
  - `LLMConfig`: LLM provider and model settings
  - `EvaluationConfig`: Evaluation parameters
  - `PhoenixConfig`: Phoenix tracing settings
  - `RAGEvalConfig`: Main configuration combining all settings
- **Methods:**
  - `from_defaults()`: Load default configuration
  - `from_env()`: Load configuration from environment variables

#### `utils.py` (Utilities)
- **Functions:**
  - `load_queries_from_file()`: Load queries from text file
  - `load_ground_truth_from_csv()`: Load labeled data from CSV
  - `save_results_to_json()`: Export results to JSON
  - `calculate_metrics_by_query()`: Per-query metric analysis
  - `analyze_error_patterns()`: Error analysis (FP/FN patterns)
  - `export_results_to_html()`: Generate HTML reports
  - `print_metrics_summary()`: Pretty-print metrics

### Testing & Quality

#### `test_evaluator.py` (Tests)
- Unit tests for all major components
- Integration tests for end-to-end workflow
- Mock LLM for deterministic testing
- Coverage: ~90%+

#### `.github/workflows/ci.yml` (CI/CD)
- Automated testing on push/PR
- Multi-OS testing (Ubuntu, macOS, Windows)
- Multi-Python version (3.8, 3.9, 3.10, 3.11)
- Code coverage reporting to Codecov
- Security scanning with Bandit

### Docker & Deployment

#### `Dockerfile`
- Python 3.10 slim base image
- Installs all dependencies
- Exposes Phoenix port 6006
- Health check included

#### `docker-compose.yml`
- Main evaluation service
- Optional Jupyter notebook service
- Volume mounts for data and results
- Environment variable configuration

### Package Management

#### `requirements.txt`
Core dependencies:
- `langchain`, `langchain-together`, `together`
- `arize-phoenix`
- `pandas`, `numpy`, `scikit-learn`
- `chromadb`, `sentence-transformers` (optional)

#### `setup.py`
- Package metadata and installation
- Entry points for CLI commands:
  - `rag-eval`: Run main evaluation
  - `rag-eval-examples`: Run examples
- Development dependencies specification

#### `Makefile`
Convenient commands:
- `make setup`: Complete setup
- `make test`: Run tests
- `make run`: Run evaluation
- `make clean`: Clean temporary files
- `make format`: Format code

### Example & Usage

#### `example_usage.py`
Six comprehensive examples:
1. Basic evaluation
2. Phoenix tracing integration
3. Error analysis
4. Custom configuration
5. Export reports (HTML, JSON, CSV)
6. Batch processing

## Key Dependencies

| Package | Version | Purpose |
|---------|---------|---------|
| langchain | â‰¥0.1.0 | LLM orchestration |
| langchain-together | â‰¥0.0.1 | Together AI integration |
| arize-phoenix | â‰¥4.0.0 | Observability and tracing |
| pandas | â‰¥2.0.0 | Data manipulation |
| scikit-learn | â‰¥1.3.0 | Metrics calculation |
| together | â‰¥1.0.0 | Together AI client |

## Data Flow

```
1. Input Data
   â””â”€â”€ queries.txt, ground_truth.csv, or Python lists

2. RAGRetrievalEvaluator
   â”œâ”€â”€ LangChain + Together AI for relevance scoring
   â””â”€â”€ Binary classification (0 or 1) per chunk

3. Metrics Calculation
   â”œâ”€â”€ Precision, Recall, Accuracy, F1
   â””â”€â”€ Confusion matrix

4. Output
   â”œâ”€â”€ CSV files (results + metrics)
   â”œâ”€â”€ HTML report
   â”œâ”€â”€ JSON export
   â””â”€â”€ Phoenix UI traces
```

## Setup Instructions

### Basic Setup
```bash
# 1. Clone and navigate
git clone <repo-url>
cd rag-evaluation

# 2. Install
make setup

# 3. Configure
cp .env.example .env
# Edit .env with your API key

# 4. Run
make run
```

### Docker Setup
```bash
# 1. Build
docker-compose build

# 2. Configure
cp .env.example .env
# Edit .env with your API key

# 3. Run
docker-compose up
```

## Development Workflow

```bash
# 1. Create feature branch
git checkout -b feature/my-feature

# 2. Make changes
vim evaluator.py

# 3. Format code
make format

# 4. Run tests
make test

# 5. Check everything
make check  # Runs lint, type-check, and test

# 6. Commit and push
git commit -am "Add feature"
git push origin feature/my-feature
```

## Common Customizations

### Add New Metric
Edit `utils.py`:
```python
def calculate_custom_metric(results):
    # Your metric calculation
    pass
```

### Change LLM Provider
Edit `config.py` or `.env`:
```python
LLM_PROVIDER=openai
OPENAI_API_KEY=your_key
```

### Custom Relevance Prompt
Edit `evaluator.py`:
```python
self.relevance_prompt = PromptTemplate(
    template="Your custom prompt",
    input_variables=["query", "document"]
)
```

## Output Formats

### CSV Format
```csv
query,retrieved_chunk,relevance_score,is_relevant,ground_truth
"What is AI?","AI is artificial intelligence",0.95,1,1
```

### JSON Format
```json
{
  "query": "What is AI?",
  "retrieved_chunk": "AI is artificial intelligence",
  "relevance_score": 0.95,
  "is_relevant": 1,
  "ground_truth": 1
}
```

### HTML Report
Interactive HTML with:
- Metrics dashboard
- Detailed results table
- Sortable columns
- Responsive design

## Version Control

### `.gitignore` (Recommended)
```gitignore
# Python
__pycache__/
*.pyc
*.pyo
*.pyd
.Python
venv/
.env

# Results
results/*.csv
results/*.json
results/*.html

# IDE
.vscode/
.idea/
*.swp

# OS
.DS_Store
Thumbs.db
```

## Additional Resources

- **Phoenix Docs**: https://docs.arize.com/phoenix
- **LangChain Docs**: https://python.langchain.com
- **Together AI Docs**: https://docs.together.ai
- **scikit-learn Metrics**: https://scikit-learn.org/stable/modules/model_evaluation.html

---

**Need help?** Open an issue or check the QUICKSTART.md guide!